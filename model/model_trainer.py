# model/model_trainer.py (å°ˆæ¥­åˆ†é¡æ¨¡å‹å‡ç´šç‰ˆ)

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
from imblearn.over_sampling import SMOTE
import numpy as np
import joblib
import os
import logging
from typing import Tuple, List
import json
import glob

# --- é…ç½®æ—¥èªŒ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- è·¯å¾‘è¨­ç½® ---
MODEL_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(MODEL_DIR)
DATA_DIR = os.path.join(PROJECT_ROOT, 'data')

import sys
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

def preprocess_for_training(filepath: str, line_type: str) -> Tuple[pd.DataFrame, List[str], OneHotEncoder, StandardScaler]:
    """
    ã€âœ¨æ ¸å¿ƒç‰¹å¾µå·¥ç¨‹å‡ç´š 2.0âœ¨ã€‘
    å¾åŸå§‹ CSV è®€å–è³‡æ–™ï¼Œå‰µå»ºæ›´è±å¯Œçš„æ™‚é–“èˆ‡ç©ºé–“ç‰¹å¾µï¼Œä¸¦ç‚ºåˆ†é¡ä»»å‹™åšæº–å‚™ã€‚
    """
    logger.info(f"--- é–‹å§‹é è™•ç† {line_type} è³‡æ–™å¾ {filepath} ---")
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨: {filepath}ã€‚è«‹å…ˆåŸ·è¡Œ data_collector.pyã€‚")
    
    df = pd.read_csv(filepath)
    if df.empty:
        raise ValueError(f"{filepath} ç‚ºç©ºï¼Œç„¡æ³•é€²è¡Œè¨“ç·´ã€‚")

    # 1. è³‡æ–™æ ¼å¼è½‰æ› (Wide to Long) - ç¶­æŒä¸è®Š
    num_cars = 4 if line_type == 'wenhu' else 6
    value_vars = [f'car{i}_congestion' for i in range(1, num_cars + 1)]
    id_vars = ['timestamp', 'station_id', 'line_direction_cid']
    
    df_melted = df.melt(id_vars=id_vars, value_vars=value_vars, var_name='car_position', value_name='congestion')
    df_melted['car_number'] = df_melted['car_position'].str.extract(r'(\d+)').astype(int)
    
    # æ¸…ç†ç›®æ¨™è®Šæ•¸ï¼šç¢ºä¿æ“æ“ åº¦æ˜¯ 1, 2, 3, 4 å…¶ä¸­ä¹‹ä¸€
    df_melted['congestion'] = pd.to_numeric(df_melted['congestion'], errors='coerce')
    df_melted.dropna(subset=['congestion'], inplace=True)
    df_melted = df_melted[df_melted['congestion'].isin([1, 2, 3, 4])].astype({'congestion': int})

    # --- ã€ âœ¨ ç‰¹å¾µå·¥ç¨‹ 2.0 - å°å…¥å°ˆå®¶çŸ¥è­˜ âœ¨ ã€‘ ---
    logger.info("      -> æ­£åœ¨å‰µå»º 2.0 ç‰ˆç‰¹å¾µ...")
    df_melted['timestamp'] = pd.to_datetime(df_melted['timestamp'])
    
    # (A) æ›´è±å¯Œçš„æ™‚é–“ç‰¹å¾µ
    df_melted['hour'] = df_melted['timestamp'].dt.hour
    # ã€æ–°å¢ã€‘æ›´ç´°ç²’åº¦çš„æ™‚é–“ç‰¹å¾µ
    df_melted['minute'] = df_melted['timestamp'].dt.minute
    df_melted['day_of_week'] = df_melted['timestamp'].dt.dayofweek
    df_melted['is_weekend'] = (df_melted['day_of_week'] >= 5).astype(int)
    df_melted['is_peak_hour'] = df_melted['hour'].isin([7, 8, 17, 18, 19]).astype(int)

    # (B) çµåˆæ·é‹è·¯ç¶²çš„ç©ºé–“ç‰¹å¾µ (Domain Knowledge)
    with open(os.path.join(DATA_DIR, 'mrt_station_info.json'), 'r', encoding='utf-8') as f:
        station_info = json.load(f)
    
    transfer_stations = {sid for info in station_info.values() if isinstance(info, dict) for sid in info.get('station_ids', []) if info.get('is_transfer')}
    df_melted['is_transfer_station'] = df_melted['station_id'].isin(transfer_stations).astype(int)
    
    # (C) æ»¯å¾Œç‰¹å¾µ (ç¶­æŒä¸è®Šï¼Œä½†æœªä¾†å¯å¼·åŒ–)
    df_melted = df_melted.sort_values(by=['station_id', 'line_direction_cid', 'car_number', 'timestamp'])
    df_melted['lag_5min_congestion'] = df_melted.groupby(['station_id', 'line_direction_cid', 'car_number'])['congestion'].shift(1)
    df_melted['lag_1hr_congestion'] = df_melted.groupby(['station_id', 'line_direction_cid', 'car_number'])['congestion'].shift(12)
    df_melted.fillna(0, inplace=True)
    
    # 3. é¡åˆ¥ç‰¹å¾µç·¨ç¢¼ - ç¶­æŒä¸è®Š
    categorical_features = ['station_id', 'line_direction_cid']
    df_melted[categorical_features] = df_melted[categorical_features].astype(str)
    
    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    encoded_data = encoder.fit_transform(df_melted[categorical_features])
    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_features), index=df_melted.index)
    
    # 4. çµ„åˆæœ€çµ‚ç‰¹å¾µ
    numeric_features = [
        'hour', 'minute', 'day_of_week', 'is_weekend', 'is_peak_hour', 'is_transfer_station',
        'car_number', 'lag_5min_congestion', 'lag_1hr_congestion'
    ]
    
    final_df = pd.concat([df_melted[numeric_features].reset_index(drop=True), encoded_df.reset_index(drop=True), df_melted['congestion'].reset_index(drop=True)], axis=1)
    feature_columns = numeric_features + list(encoder.get_feature_names_out(categorical_features))
    
    # ã€æ–°å¢ã€‘ç‰¹å¾µç¸®æ”¾
    scaler = StandardScaler()
    final_df[numeric_features] = scaler.fit_transform(final_df[numeric_features])
    
    logger.info(f"--- é è™•ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(final_df)} ç­†æœ‰æ•ˆè¨“ç·´æ¨£æœ¬ï¼Œä½¿ç”¨ {len(feature_columns)} å€‹ç‰¹å¾µã€‚")
    return final_df, feature_columns, encoder, scaler

def train_and_save_model(df: pd.DataFrame, feature_columns: list, line_type: str, encoder: OneHotEncoder, scaler: StandardScaler):
    """
    ã€âœ¨æ¨¡å‹è¨“ç·´èˆ‡èª¿å„ªå‡ç´šâœ¨ã€‘
    ä½¿ç”¨ GridSearchCV é€²è¡Œè¶…åƒæ•¸èª¿å„ªï¼Œä¸¦ä½¿ç”¨åˆ†é¡è©•ä¼°æŒ‡æ¨™ã€‚
    """
    logger.info(f"--- é–‹å§‹è¨“ç·´ {line_type} åˆ†é¡æ¨¡å‹... ---")
    
    X = df[feature_columns]
    y = df['congestion'] - 1 # ç›®æ¨™è®Šæ•¸è½‰æ›ç‚º 0-indexed
    
    # ä½¿ç”¨ stratify=y ç¢ºä¿è¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¸­çš„å„é¡åˆ¥æ¯”ä¾‹èˆ‡åŸå§‹æ•¸æ“šç›¸åŒ
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # ã€æ–°å¢ã€‘è™•ç†é¡åˆ¥ä¸å¹³è¡¡å•é¡Œ (è‹¥éœ€è¦)
    # æª¢æŸ¥ y_train çš„é¡åˆ¥åˆ†ä½ˆï¼Œå¦‚æœæŸå€‹é¡åˆ¥æ¨£æœ¬éå°‘ï¼Œå¯ä»¥å•Ÿç”¨ SMOTE
    # logger.info(f"è¨“ç·´é›†é¡åˆ¥åˆ†ä½ˆ: {np.bincount(y_train)}")
    # smote = SMOTE(random_state=42)
    # X_train, y_train = smote.fit_resample(X_train, y_train)
    # logger.info(f"SMOTE è™•ç†å¾Œè¨“ç·´é›†é¡åˆ¥åˆ†ä½ˆ: {np.bincount(y_train)}")

    # ã€æ–°å¢ã€‘ä½¿ç”¨ GridSearchCV é€²è¡Œè¶…åƒæ•¸èª¿å„ª
    logger.info("--- âš™ï¸ é–‹å§‹ä½¿ç”¨ GridSearchCV é€²è¡Œè¶…åƒæ•¸èª¿å„ª... ---")
    param_grid = {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.05, 0.1, 0.2],
        'max_depth': [3, 5, 7],
        'subsample': [0.8],
        'colsample_bytree': [0.8]
    }
    
    # ä½¿ç”¨ TimeSeriesSplit é€²è¡Œäº¤å‰é©—è­‰
    tscv = TimeSeriesSplit(n_splits=5)
    
    xgb_model = xgb.XGBClassifier(
        objective='multi:softmax',
        num_class=4,
        random_state=42,
        n_jobs=-1,
        eval_metric='mlogloss',
        use_label_encoder=False,
    )
    
    grid_search = GridSearchCV(
        estimator=xgb_model,
        param_grid=param_grid,
        scoring='accuracy',
        cv=tscv,
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    
    logger.info(f"--- âœ… è¶…åƒæ•¸èª¿å„ªå®Œæˆï¼Œæ‰¾åˆ°æœ€ä½³åƒæ•¸çµ„åˆ: {grid_search.best_params_} ---")

    # ä½¿ç”¨æœ€ä½³æ¨¡å‹é€²è¡Œé æ¸¬
    y_pred = best_model.predict(X_test)
    y_pred_proba = best_model.predict_proba(X_test)
    
    # --- ã€ âœ¨ æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨åˆ†é¡è©•ä¼°æŒ‡æ¨™ï¼Œä¸¦æ“´å…… AUC-ROC âœ¨ ã€‘ ---
    accuracy = accuracy_score(y_test, y_pred)
    logger.info(f"--- âœ… {line_type} æ¨¡å‹è¨“ç·´å®Œæˆï¼Œè©•ä¼° Accuracy (æº–ç¢ºç‡): {accuracy:.4f} ---")
    
    # æ‰“å°æ›´è©³ç´°çš„åˆ†é¡å ±å‘Š
    report = classification_report(y_test, y_pred, target_names=['èˆ’é©(1)', 'æ­£å¸¸(2)', 'ç•¥å¤š(3)', 'æ“æ“ (4)'])
    logger.info(f"\n--- åˆ†é¡å ±å‘Š ({line_type}) ---\n{report}")
    
    # è¨ˆç®— AUC-ROC
    auc_roc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
    logger.info(f"--- âœ… {line_type} æ¨¡å‹çš„ AUC-ROC: {auc_roc:.4f} ---")
    
    # å„²å­˜ç”¢ç‰©
    output_dir = MODEL_DIR 
    best_model.save_model(os.path.join(output_dir, f'{line_type}_congestion_model.json'))
    joblib.dump(encoder, os.path.join(output_dir, f'{line_type}_encoder.joblib'))
    joblib.dump(scaler, os.path.join(output_dir, f'{line_type}_scaler.joblib')) # ã€æ–°å¢ã€‘å„²å­˜ scaler
    pd.DataFrame(feature_columns, columns=['feature']).to_csv(os.path.join(output_dir, f'{line_type}_feature_columns.csv'), index=False)
    
    logger.info(f"      -> æ¨¡å‹ç›¸é—œç”¢ç‰©å·²ä¿å­˜è‡³: {output_dir}")

if __name__ == "__main__":
    logger.warning("--- æº–å‚™é–‹å§‹æ–°ä¸€è¼ªçš„ã€åˆ†é¡æ¨¡å‹ã€è¨“ç·´ã€‚å·²æ–°å¢è‡ªå‹•åˆªé™¤èˆŠæ¨¡å‹æª”æ¡ˆåŠŸèƒ½ï¼ ---")
    
    # ã€æ–°å¢åŠŸèƒ½ã€‘è‡ªå‹•åˆªé™¤èˆŠæ¨¡å‹æª”æ¡ˆ
    # å°‹æ‰¾æ‰€æœ‰ line_type_congestion_model.jsonã€_encoder.joblibã€_feature_columns.csvã€_scaler.joblib æª”æ¡ˆ
    old_files = glob.glob(os.path.join(MODEL_DIR, '*_congestion_model.json')) + \
                glob.glob(os.path.join(MODEL_DIR, '*_encoder.joblib')) + \
                glob.glob(os.path.join(MODEL_DIR, '*_feature_columns.csv')) + \
                glob.glob(os.path.join(MODEL_DIR, '*_scaler.joblib'))
    
    if old_files:
        logger.info(f"--- ğŸ—‘ï¸ æ­£åœ¨åˆªé™¤ {len(old_files)} å€‹èˆŠæ¨¡å‹æª”æ¡ˆ... ---")
        for file_path in old_files:
            try:
                os.remove(file_path)
                logger.info(f"      -> å·²åˆªé™¤: {os.path.basename(file_path)}")
            except OSError as e:
                logger.error(f"åˆªé™¤æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    for line_type in ['high_capacity', 'wenhu']:
        filepath = os.path.join(DATA_DIR, f'{line_type}_congestion.csv')
        try:
            # ç¢ºä¿å‡½æ•¸å‘¼å«å¯ä»¥æ¥æ”¶æ–°å¢çš„å›å‚³å€¼
            processed_df, features, fitted_encoder, fitted_scaler = preprocess_for_training(filepath, line_type)
            # ç¢ºä¿å‡½æ•¸å‘¼å«å¯ä»¥å‚³éæ–°å¢çš„åƒæ•¸
            train_and_save_model(processed_df, features, line_type, fitted_encoder, fitted_scaler)
        except (FileNotFoundError, ValueError) as e:
            logger.error(f"--- âŒ {line_type} è¨“ç·´å¤±æ•—: {e} ---")
            logger.error("è«‹ç¢ºä¿å·²é‹è¡Œ data_collector.py ä¸¦æ”¶é›†åˆ°è¶³å¤ çš„è³‡æ–™ã€‚")
        except Exception as e:
            logger.critical(f"--- âŒ {line_type} è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸæœªçŸ¥åš´é‡éŒ¯èª¤: {e} ---", exc_info=True)
    
    logger.info("\n--- ğŸ‰ æ‰€æœ‰æ¨¡å‹è¨“ç·´æµç¨‹çµæŸï¼ ---")