import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
import numpy as np
import joblib
import os
import logging
from typing import Tuple, List

# --- é…ç½®æ—¥èªŒ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- å‹•æ…‹è·¯å¾‘è¨­ç½® ---
import sys
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.append(project_root)

def preprocess_for_training(filepath: str, line_type: str) -> Tuple[pd.DataFrame, List[str], OneHotEncoder]:
    """
    ã€æ ¸å¿ƒç‰¹å¾µå·¥ç¨‹ã€‘
    å¾ä¹¾æ·¨çš„ CSV è®€å–è³‡æ–™ï¼Œå‰µå»ºæ™‚é–“ç‰¹å¾µã€æ»¯å¾Œç‰¹å¾µï¼Œä¸¦é€²è¡Œç·¨ç¢¼ã€‚
    """
    logger.info(f"--- é–‹å§‹é è™•ç† {line_type} è³‡æ–™å¾ {filepath} ---")
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨: {filepath}ã€‚è«‹å…ˆåŸ·è¡Œ data_collector.pyã€‚")
    
    df = pd.read_csv(filepath)
    if df.empty:
        raise ValueError(f"{filepath} ç‚ºç©ºï¼Œç„¡æ³•é€²è¡Œè¨“ç·´ã€‚")

    # 1. è³‡æ–™æ ¼å¼è½‰æ› (Wide to Long)
    # å°‡æ¯å€‹è»Šå»‚çš„æ“æ“ åº¦è®Šæˆç¨ç«‹çš„è¡Œï¼Œé€™æ¨£æ¨¡å‹å°±èƒ½å­¸ç¿’ã€Œè»Šå»‚ä½ç½®ã€é€™å€‹ç‰¹å¾µ
    num_cars = 4 if line_type == 'wenhu' else 6
    value_vars = [f'car{i}_congestion' for i in range(1, num_cars + 1)]
    id_vars = ['timestamp', 'station_id', 'line_direction_cid']
    
    # ç¢ºä¿æ‰€æœ‰å¿…è¦çš„æ¬„ä½éƒ½å­˜åœ¨
    for col in id_vars + value_vars:
        if col not in df.columns:
            raise ValueError(f"CSV æª”æ¡ˆ {filepath} ç¼ºå°‘å¿…è¦æ¬„ä½: {col}")

    df_melted = df.melt(id_vars=id_vars, value_vars=value_vars, var_name='car_position', value_name='congestion')
    df_melted['car_number'] = df_melted['car_position'].str.extract(r'(\d+)').astype(int)
    
    # æ¸…ç†æ‰æ“æ“ åº¦ç‚ºç©ºçš„è¨˜éŒ„ (ä¾‹å¦‚æ–‡æ¹–ç·šçš„ car5, car6)
    df_melted.dropna(subset=['congestion'], inplace=True)
    df_melted['congestion'] = pd.to_numeric(df_melted['congestion'], errors='coerce')
    df_melted.dropna(subset=['congestion'], inplace=True)

    # 2. ç‰¹å¾µå·¥ç¨‹
    df_melted['timestamp'] = pd.to_datetime(df_melted['timestamp'])
    df_melted['hour'] = df_melted['timestamp'].dt.hour
    df_melted['day_of_week'] = df_melted['timestamp'].dt.dayofweek
    df_melted['is_weekend'] = (df_melted['day_of_week'] >= 5).astype(int)
    
    # æ»¯å¾Œç‰¹å¾µ (Lag Features) - é€™æ˜¯é æ¸¬æ™‚é–“åºåˆ—çš„é—œéµ
    logger.info("    -> æ­£åœ¨å‰µå»ºæ»¯å¾Œç‰¹å¾µ...")
    df_melted = df_melted.sort_values(by=['station_id', 'line_direction_cid', 'car_number', 'timestamp'])
    # ä¸Šä¸€å€‹æ™‚é–“é»çš„æ“æ“ åº¦ (5åˆ†é˜å‰)
    df_melted['lag_5min_congestion'] = df_melted.groupby(['station_id', 'line_direction_cid', 'car_number'])['congestion'].shift(1)
    # ä¸€å°æ™‚å‰çš„æ“æ“ åº¦ (å‡è¨­5åˆ†é˜æ”¶é›†ä¸€æ¬¡ï¼Œ12*5=60)
    df_melted['lag_1hr_congestion'] = df_melted.groupby(['station_id', 'line_direction_cid', 'car_number'])['congestion'].shift(12)
    
    # ã€é—œéµä¿®æ­£ã€‘ç”¨ 0 å¡«å……æ»¯å¾Œç‰¹å¾µçš„ç¼ºå¤±å€¼ï¼Œè€Œä¸æ˜¯åˆªé™¤æ•´è¡Œ
    df_melted.fillna({
        'lag_5min_congestion': 0,
        'lag_1hr_congestion': 0
    }, inplace=True)
    
    # 3. é¡åˆ¥ç‰¹å¾µç·¨ç¢¼ (One-Hot Encoding)
    categorical_features = ['station_id', 'line_direction_cid']
    df_melted[categorical_features] = df_melted[categorical_features].astype(str)
    
    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    encoded_data = encoder.fit_transform(df_melted[categorical_features])
    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_features), index=df_melted.index)
    
    # 4. çµ„åˆæœ€çµ‚ç‰¹å¾µ
    numeric_features = ['hour', 'day_of_week', 'is_weekend', 'car_number', 'lag_5min_congestion', 'lag_1hr_congestion']
    final_df = pd.concat([df_melted[numeric_features], encoded_df, df_melted['congestion']], axis=1)
    feature_columns = numeric_features + list(encoder.get_feature_names_out(categorical_features))
    
    logger.info(f"--- é è™•ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(final_df)} ç­†æœ‰æ•ˆè¨“ç·´æ¨£æœ¬ã€‚")
    return final_df, feature_columns, encoder

def train_and_save_model(df: pd.DataFrame, feature_columns: list, line_type: str, encoder: OneHotEncoder):
    """è¨“ç·´ XGBoost æ¨¡å‹ä¸¦ä¿å­˜æ‰€æœ‰å¿…è¦çš„ç”¢ç‰©ã€‚"""
    logger.info(f"--- é–‹å§‹è¨“ç·´ {line_type} æ¨¡å‹... ---")
    
    X = df[feature_columns]
    y = df['congestion']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=1000,
        learning_rate=0.05,
        max_depth=7,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        early_stopping_rounds=50, # æå‰åœæ­¢ä»¥é˜²éæ“¬åˆ
        n_jobs=-1 # ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ
    )
    
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
    
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)  # é¦–å…ˆè¨ˆç®— MSE
    rmse = np.sqrt(mse)                      # å†å° MSE é–‹æ ¹è™Ÿå¾—åˆ° RMSE
    # ã€æ–°å¢é€™è¡Œã€‘è¨ˆç®— MAPE
    mape = mean_absolute_percentage_error(y_test, y_pred)
    logger.info(f"--- âœ… {line_type} æ¨¡å‹è¨“ç·´å®Œæˆï¼Œè©•ä¼° RMSE: {rmse:.4f}, MAPE: {mape:.4f} ---")
    
    # 5. ä¿å­˜æ‰€æœ‰ç”¢ç‰© (æ¨¡å‹ã€ç·¨ç¢¼å™¨ã€ç‰¹å¾µåˆ—è¡¨)
    data_dir = os.path.join(project_root, 'data')
    model.save_model(os.path.join(data_dir, f'{line_type}_congestion_model.json'))
    joblib.dump(encoder, os.path.join(data_dir, f'{line_type}_encoder.joblib'))
    pd.DataFrame(feature_columns, columns=['feature']).to_csv(os.path.join(data_dir, f'{line_type}_feature_columns.csv'), index=False)
    
    logger.info(f"    -> æ¨¡å‹å·²ä¿å­˜è‡³: {line_type}_congestion_model.json")
    logger.info(f"    -> ç·¨ç¢¼å™¨å·²ä¿å­˜è‡³: {line_type}_encoder.joblib")
    logger.info(f"    -> ç‰¹å¾µåˆ—è¡¨å·²ä¿å­˜è‡³: {line_type}_feature_columns.csv")

if __name__ == "__main__":
    # å°é«˜é‹é‡ç·šå’Œæ–‡æ¹–ç·šåˆ†åˆ¥é€²è¡Œè¨“ç·´
    for line_type in ['high_capacity', 'wenhu']:
        filepath = os.path.join(project_root, 'data', f'{line_type}_congestion.csv')
        try:
            processed_df, features, fitted_encoder = preprocess_for_training(filepath, line_type)
            train_and_save_model(processed_df, features, line_type, fitted_encoder)
        except (FileNotFoundError, ValueError) as e:
            logger.error(f"--- âŒ {line_type} è¨“ç·´å¤±æ•—: {e} ---")
            logger.error("è«‹ç¢ºä¿å·²é‹è¡Œ data_collector.py ä¸¦æ”¶é›†åˆ°è¶³å¤ çš„è³‡æ–™ã€‚")
        except Exception as e:
            logger.critical(f"--- âŒ {line_type} è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸæœªçŸ¥åš´é‡éŒ¯èª¤: {e} ---", exc_info=True)
    
    logger.info("\n--- ğŸ‰ æ‰€æœ‰æ¨¡å‹è¨“ç·´æµç¨‹çµæŸï¼ ---")