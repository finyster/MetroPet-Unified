# model/model_trainer.py (å°ˆæ¥­åˆ†é¡æ¨¡å‹å‡ç´šç‰ˆ)

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score, classification_report # åŒ¯å…¥åˆ†é¡æ¨¡å‹çš„è©•ä¼°å·¥å…·
import numpy as np
import joblib
import os
import logging
from typing import Tuple, List
import json

# --- é…ç½®æ—¥èªŒ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- è·¯å¾‘è¨­ç½® ---
MODEL_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(MODEL_DIR)
DATA_DIR = os.path.join(PROJECT_ROOT, 'data')

import sys
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

def preprocess_for_training(filepath: str, line_type: str) -> Tuple[pd.DataFrame, List[str], OneHotEncoder]:
    """
    ã€âœ¨æ ¸å¿ƒç‰¹å¾µå·¥ç¨‹å‡ç´š 2.0âœ¨ã€‘
    å¾åŸå§‹ CSV è®€å–è³‡æ–™ï¼Œå‰µå»ºæ›´è±å¯Œçš„æ™‚é–“èˆ‡ç©ºé–“ç‰¹å¾µï¼Œä¸¦ç‚ºåˆ†é¡ä»»å‹™åšæº–å‚™ã€‚
    """
    logger.info(f"--- é–‹å§‹é è™•ç† {line_type} è³‡æ–™å¾ {filepath} ---")
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨: {filepath}ã€‚è«‹å…ˆåŸ·è¡Œ data_collector.pyã€‚")
    
    df = pd.read_csv(filepath)
    if df.empty:
        raise ValueError(f"{filepath} ç‚ºç©ºï¼Œç„¡æ³•é€²è¡Œè¨“ç·´ã€‚")

    # 1. è³‡æ–™æ ¼å¼è½‰æ› (Wide to Long) - ç¶­æŒä¸è®Š
    num_cars = 4 if line_type == 'wenhu' else 6
    value_vars = [f'car{i}_congestion' for i in range(1, num_cars + 1)]
    id_vars = ['timestamp', 'station_id', 'line_direction_cid']
    
    df_melted = df.melt(id_vars=id_vars, value_vars=value_vars, var_name='car_position', value_name='congestion')
    df_melted['car_number'] = df_melted['car_position'].str.extract(r'(\d+)').astype(int)
    
    # æ¸…ç†ç›®æ¨™è®Šæ•¸ï¼šç¢ºä¿æ“æ“ åº¦æ˜¯ 1, 2, 3, 4 å…¶ä¸­ä¹‹ä¸€
    df_melted['congestion'] = pd.to_numeric(df_melted['congestion'], errors='coerce')
    df_melted.dropna(subset=['congestion'], inplace=True)
    df_melted = df_melted[df_melted['congestion'].isin([1, 2, 3, 4])].astype({'congestion': int})

    # --- ã€ âœ¨ ç‰¹å¾µå·¥ç¨‹ 2.0 - å°å…¥å°ˆå®¶çŸ¥è­˜ âœ¨ ã€‘ ---
    logger.info("      -> æ­£åœ¨å‰µå»º 2.0 ç‰ˆç‰¹å¾µ...")
    df_melted['timestamp'] = pd.to_datetime(df_melted['timestamp'])
    
    # (A) æ›´è±å¯Œçš„æ™‚é–“ç‰¹å¾µ
    df_melted['hour'] = df_melted['timestamp'].dt.hour
    df_melted['day_of_week'] = df_melted['timestamp'].dt.dayofweek
    df_melted['is_weekend'] = (df_melted['day_of_week'] >= 5).astype(int)
    # æ–°å¢ï¼šæ˜¯å¦ç‚ºå°–å³°æ™‚æ®µ (æ—©ä¸Š 7-9 é», å‚æ™š 17-19 é»)ï¼Œé€™æ˜¯å½±éŸ¿äººæµçš„é—œéµå› å­
    df_melted['is_peak_hour'] = df_melted['hour'].isin([7, 8, 17, 18, 19]).astype(int)

    # (B) çµåˆæ·é‹è·¯ç¶²çš„ç©ºé–“ç‰¹å¾µ (Domain Knowledge)
    with open(os.path.join(DATA_DIR, 'mrt_station_info.json'), 'r', encoding='utf-8') as f:
        station_info = json.load(f)
    
    # æœ€çµ‚ä¿®æ­£ï¼šéæ­·å­—å…¸çš„ .values()ï¼Œä¸¦ç”¨ isinstance æª¢æŸ¥ç¢ºä¿å¥å£¯æ€§
    transfer_stations = {sid for info in station_info.values() if isinstance(info, dict) for sid in info.get('station_ids', []) if info.get('is_transfer')}
    df_melted['is_transfer_station'] = df_melted['station_id'].isin(transfer_stations).astype(int)
    
    # (C) æ»¯å¾Œç‰¹å¾µ (ç¶­æŒä¸è®Šï¼Œä½†æœªä¾†å¯å¼·åŒ–)
    df_melted = df_melted.sort_values(by=['station_id', 'line_direction_cid', 'car_number', 'timestamp'])
    df_melted['lag_5min_congestion'] = df_melted.groupby(['station_id', 'line_direction_cid', 'car_number'])['congestion'].shift(1)
    df_melted['lag_1hr_congestion'] = df_melted.groupby(['station_id', 'line_direction_cid', 'car_number'])['congestion'].shift(12)
    df_melted.fillna(0, inplace=True)
    
    # 3. é¡åˆ¥ç‰¹å¾µç·¨ç¢¼ - ç¶­æŒä¸è®Š
    categorical_features = ['station_id', 'line_direction_cid']
    df_melted[categorical_features] = df_melted[categorical_features].astype(str)
    
    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    encoded_data = encoder.fit_transform(df_melted[categorical_features])
    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_features), index=df_melted.index)
    
    # 4. çµ„åˆæœ€çµ‚ç‰¹å¾µ
    numeric_features = [
        'hour', 'day_of_week', 'is_weekend', 'is_peak_hour', 'is_transfer_station',
        'car_number', 'lag_5min_congestion', 'lag_1hr_congestion'
    ]
    final_df = pd.concat([df_melted[numeric_features].reset_index(drop=True), encoded_df.reset_index(drop=True), df_melted['congestion'].reset_index(drop=True)], axis=1)
    feature_columns = numeric_features + list(encoder.get_feature_names_out(categorical_features))
    
    logger.info(f"--- é è™•ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(final_df)} ç­†æœ‰æ•ˆè¨“ç·´æ¨£æœ¬ï¼Œä½¿ç”¨ {len(feature_columns)} å€‹ç‰¹å¾µã€‚")
    return final_df, feature_columns, encoder

def train_and_save_model(df: pd.DataFrame, feature_columns: list, line_type: str, encoder: OneHotEncoder):
    """
    ã€âœ¨æ¨¡å‹è¨“ç·´å‡ç´šâœ¨ã€‘
    ä½¿ç”¨ XGBoost åˆ†é¡å™¨ï¼Œä¸¦è©•ä¼°åˆ†é¡æ¨¡å‹çš„æ•ˆèƒ½æŒ‡æ¨™ã€‚
    """
    logger.info(f"--- é–‹å§‹è¨“ç·´ {line_type} åˆ†é¡æ¨¡å‹... ---")
    
    X = df[feature_columns]
    
    # --- ã€ âœ¨ æ ¸å¿ƒä¿®æ”¹ï¼šç›®æ¨™è®Šæ•¸è½‰æ›ç‚º 0-indexed âœ¨ ã€‘ ---
    # åŸå§‹æ¨™ç±¤æ˜¯ 1, 2, 3, 4ã€‚XGBoost åˆ†é¡å™¨éœ€è¦å¾ 0 é–‹å§‹çš„æ¨™ç±¤ã€‚
    # æ‰€ä»¥æˆ‘å€‘å°‡æ‰€æœ‰æ¨™ç±¤æ¸› 1ï¼Œè®Šæˆ 0, 1, 2, 3ã€‚
    y = df['congestion'] - 1
    
    # ä½¿ç”¨ stratify=y ç¢ºä¿è¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¸­çš„å„é¡åˆ¥æ¯”ä¾‹èˆ‡åŸå§‹æ•¸æ“šç›¸åŒ
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # --- ã€ âœ¨ æ ¸å¿ƒä¿®æ”¹ï¼šæ›´æ›ç‚º XGBClassifier åˆ†é¡æ¨¡å‹ âœ¨ ã€‘ ---
    model = xgb.XGBClassifier(
        objective='multi:softmax',  # ç›®æ¨™å‡½æ•¸æ”¹ç‚ºå¤šåˆ†é¡
        num_class=4,                # å‘ŠçŸ¥æ¨¡å‹ç¸½å…±æœ‰ 4 å€‹é¡åˆ¥ (0, 1, 2, 3)
        n_estimators=500,           # æ¸›å°‘æ¨¹çš„æ•¸é‡ï¼Œç”¨ early_stopping ä¾†æ‰¾æœ€ä½³é»
        learning_rate=0.1,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        early_stopping_rounds=30,   # è¨­å®šæ—©åœæ©Ÿåˆ¶ï¼Œé˜²æ­¢éæ“¬åˆï¼Œæå‡è¨“ç·´æ•ˆç‡
        n_jobs=-1,
        eval_metric='mlogloss'      # è¨­å®šè©•ä¼°æŒ‡æ¨™
    )
    
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
    
    y_pred = model.predict(X_test)
    
    # --- ã€ âœ¨ æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨åˆ†é¡è©•ä¼°æŒ‡æ¨™ï¼Œå‘Šåˆ¥ MAPE âœ¨ ã€‘ ---
    accuracy = accuracy_score(y_test, y_pred)
    logger.info(f"--- âœ… {line_type} æ¨¡å‹è¨“ç·´å®Œæˆï¼Œè©•ä¼° Accuracy (æº–ç¢ºç‡): {accuracy:.4f} ---")
    
    # æ‰“å°æ›´è©³ç´°çš„åˆ†é¡å ±å‘Š (Precision, Recall, F1-score)ï¼Œé€™èƒ½å‘Šè¨´æˆ‘å€‘æ¨¡å‹å°æ¯å€‹æ“æ“ ç­‰ç´šçš„é æ¸¬èƒ½åŠ›
    report = classification_report(y_test, y_pred, target_names=['èˆ’é©(1)', 'æ­£å¸¸(2)', 'ç•¥å¤š(3)', 'æ“æ“ (4)'])
    logger.info(f"\n--- åˆ†é¡å ±å‘Š ({line_type}) ---\n{report}")
    
    # å„²å­˜ç”¢ç‰© - ç¶­æŒä¸è®Š
    output_dir = MODEL_DIR 
    model.save_model(os.path.join(output_dir, f'{line_type}_congestion_model.json'))
    joblib.dump(encoder, os.path.join(output_dir, f'{line_type}_encoder.joblib'))
    pd.DataFrame(feature_columns, columns=['feature']).to_csv(os.path.join(output_dir, f'{line_type}_feature_columns.csv'), index=False)
    
    logger.info(f"      -> æ¨¡å‹ç›¸é—œç”¢ç‰©å·²ä¿å­˜è‡³: {output_dir}")

if __name__ == "__main__":
    logger.warning("--- æº–å‚™é–‹å§‹æ–°ä¸€è¼ªçš„ã€åˆ†é¡æ¨¡å‹ã€è¨“ç·´ï¼Œå»ºè­°å…ˆæ‰‹å‹•åˆªé™¤ model/ è³‡æ–™å¤¾ä¸­èˆŠçš„æ¨¡å‹æª”æ¡ˆï¼ ---")
    
    for line_type in ['high_capacity', 'wenhu']:
        filepath = os.path.join(DATA_DIR, f'{line_type}_congestion.csv')
        try:
            processed_df, features, fitted_encoder = preprocess_for_training(filepath, line_type)
            train_and_save_model(processed_df, features, line_type, fitted_encoder)
        except (FileNotFoundError, ValueError) as e:
            logger.error(f"--- âŒ {line_type} è¨“ç·´å¤±æ•—: {e} ---")
            logger.error("è«‹ç¢ºä¿å·²é‹è¡Œ data_collector.py ä¸¦æ”¶é›†åˆ°è¶³å¤ çš„è³‡æ–™ã€‚")
        except Exception as e:
            logger.critical(f"--- âŒ {line_type} è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸæœªçŸ¥åš´é‡éŒ¯èª¤: {e} ---", exc_info=True)
    
    logger.info("\n--- ğŸ‰ æ‰€æœ‰æ¨¡å‹è¨“ç·´æµç¨‹çµæŸï¼ ---")
