# data_collector.py

import pandas as pd
import os
import logging
from datetime import datetime
# å‡è¨­æ‚¨çš„ service æª”æ¡ˆä½æ–¼ services/metro_soap_service.py
from services.metro_soap_service import metro_soap_api
import time

# --- (æ—¥èªŒè¨­å®šèˆ‡è·¯å¾‘å®šç¾©ï¼Œé€™éƒ¨åˆ†ç¶­æŒåŸæ¨£å³å¯) ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s,%(msecs)03d - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼Œå‡è¨­æ­¤è…³æœ¬åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(PROJECT_ROOT, 'data')

HIGH_CAPACITY_CONGESTION_FILE = os.path.join(DATA_DIR, 'high_capacity_congestion.csv')
WENHU_CONGESTION_FILE = os.path.join(DATA_DIR, 'wenhu_congestion.csv')

os.makedirs(DATA_DIR, exist_ok=True)

# ã€é—œéµé»1ã€‘å®šç¾©äº†çµ±ä¸€çš„ã€ä¹¾æ·¨çš„æ¬„ä½æ¨™æº–
FINAL_COLUMNS = [
    'timestamp', 
    'station_id', 
    'line_direction_cid',
    'car1_congestion', 
    'car2_congestion', 
    'car3_congestion', 
    'car4_congestion', 
    'car5_congestion', 
    'car6_congestion'
]

# --- (load_data å’Œ save_data å‡½æ•¸ç¶­æŒåŸæ¨£ï¼Œå®ƒå€‘çš„è¨­è¨ˆæ˜¯æ­£ç¢ºçš„) ---
def load_data(file_path: str) -> pd.DataFrame:
    if os.path.exists(file_path):
        try:
            df = pd.read_csv(file_path)
            return df.reindex(columns=FINAL_COLUMNS, fill_value=0)
        except pd.errors.EmptyDataError:
            logger.warning(f"âš ï¸ CSV æª”æ¡ˆ '{file_path}' ç‚ºç©ºï¼Œå°‡å»ºç«‹æ–°çš„ DataFrameã€‚")
            return pd.DataFrame(columns=FINAL_COLUMNS)
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥æª”æ¡ˆ '{file_path}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return pd.DataFrame(columns=FINAL_COLUMNS)
    logger.info(f"ğŸ“ æª”æ¡ˆ '{file_path}' ä¸å­˜åœ¨ï¼Œå°‡å»ºç«‹æ–°çš„ DataFrameã€‚")
    return pd.DataFrame(columns=FINAL_COLUMNS)

def save_data(df: pd.DataFrame, file_path: str):
    # ã€é—œéµé»2ã€‘å„²å­˜å‰ï¼Œå¼·åˆ¶ DataFrame ç¬¦åˆ FINAL_COLUMNS çš„çµæ§‹
    df_to_save = df.reindex(columns=FINAL_COLUMNS, fill_value=0)
    df_to_save.to_csv(file_path, index=False, mode='w', header=True)
    logger.info(f"ğŸ“Š å·²å°‡ {len(df_to_save)} ç­†è³‡æ–™å„²å­˜åˆ° {file_path}")

# --- (process_* å‡½æ•¸ç¶­æŒåŸæ¨£ï¼Œå®ƒå€‘çš„è¨­è¨ˆæ˜¯æ­£ç¢ºçš„) ---

def process_high_capacity_data(raw_data: list[dict]) -> pd.DataFrame:
    processed_records = []
    for item in raw_data:
        try:
            # ã€é—œéµé»3ã€‘å¾è¤‡é›œçš„ API å›æ‡‰ä¸­ï¼ŒåªæŒ‘é¸æˆ‘å€‘éœ€è¦çš„æ¬„ä½ï¼Œä¸¦å°æ‡‰åˆ°æ¨™æº–åç¨±
            record = {
                'timestamp': item.get('utime', ''),
                'station_id': item.get('StationID', ''),
                'line_direction_cid': int(item.get('CID', '0')) if str(item.get('CID', '0')).isdigit() else 0,
                'car1_congestion': item.get('Cart1L', '0'),
                'car2_congestion': item.get('Cart2L', '0'),
                'car3_congestion': item.get('Cart3L', '0'),
                'car4_congestion': item.get('Cart4L', '0'),
                'car5_congestion': item.get('Cart5L', '0'),
                'car6_congestion': item.get('Cart6L', '0')
            }
            if not all([record['timestamp'], record['station_id']]):
                logger.warning(f"é«˜é‹é‡ç·šè¨˜éŒ„ç¼ºå°‘å¿…è¦æ¬„ä½ (timestamp/station_id)ï¼Œè·³é: {item}")
                continue
            processed_records.append(record)
        except Exception as e:
            logger.error(f"âŒ è™•ç†é«˜é‹é‡ç·šå–®ç­†è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {item} - {e}", exc_info=True)
            continue
    df = pd.DataFrame(processed_records)
    # å†æ¬¡ç¢ºä¿æ¬„ä½çµæ§‹æ­£ç¢º
    df = df.reindex(columns=FINAL_COLUMNS, fill_value=0)
    return df

def process_wenhu_data(raw_data: list[dict]) -> pd.DataFrame:
    processed_records = []
    for item in raw_data:
        try:
            # ã€é—œéµé»4ã€‘åŒæ¨£åœ°ï¼Œè™•ç†æ–‡æ¹–ç·šè³‡æ–™ï¼Œä¸¦æ‰‹å‹•è£œä¸Šä¸å­˜åœ¨çš„è»Šå»‚
            record = {
                'timestamp': item.get('UpdateTime', ''),
                'station_id': item.get('StationID', ''),
                'line_direction_cid': int(item.get('CID', '0')) if str(item.get('CID', '0')).isdigit() else 0,
                'car1_congestion': item.get('Car1', '0'),
                'car2_congestion': item.get('Car2', '0'),
                'car3_congestion': item.get('Car3', '0'),
                'car4_congestion': item.get('Car4', '0'),
                'car5_congestion': '0', # æ–‡æ¹–ç·šå›ºå®šç‚º0
                'car6_congestion': '0'  # æ–‡æ¹–ç·šå›ºå®šç‚º0
            }
            if not all([record['timestamp'], record['station_id']]):
                logger.warning(f"æ–‡æ¹–ç·šè¨˜éŒ„ç¼ºå°‘å¿…è¦æ¬„ä½ (timestamp/station_id)ï¼Œè·³é: {item}")
                continue
            processed_records.append(record)
        except Exception as e:
            logger.error(f"âŒ è™•ç†æ–‡æ¹–ç·šå–®ç­†è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {item} - {e}", exc_info=True)
            continue
    df = pd.DataFrame(processed_records)
    df = df.reindex(columns=FINAL_COLUMNS, fill_value=0)
    return df

# --- (ä¸»é‚è¼¯ collect_and_save_congestion_data ç¶­æŒåŸæ¨£) ---
def collect_and_save_congestion_data():
    logger.info("--- [Collector] é–‹å§‹æ–°ä¸€è¼ªè³‡æ–™æ”¶é›† ---")
    
    # é«˜é‹é‡ç·š
    logger.info("--- å˜—è©¦ç²å–é«˜é‹é‡ç·šè³‡æ–™ ---")
    high_capacity_raw_data = metro_soap_api.get_high_capacity_car_weight_info()
    if high_capacity_raw_data:
        processed_df = process_high_capacity_data(high_capacity_raw_data)
        existing_df = load_data(HIGH_CAPACITY_CONGESTION_FILE)
        combined_df = pd.concat([existing_df, processed_df]).drop_duplicates(subset=['timestamp', 'station_id', 'line_direction_cid'], keep='last').reset_index(drop=True)
        save_data(combined_df, HIGH_CAPACITY_CONGESTION_FILE)
        logger.info(f" Â  Â -> âœ… é«˜é‹é‡ç·šè™•ç†å®Œç•¢ã€‚æ–°ç²å– {len(processed_df)} ç­†ï¼Œç›®å‰ç¸½å…± {len(combined_df)} ç­†ã€‚")
    else:
        logger.error(" Â  Â -> âŒ å¾ API ç²å–é«˜é‹é‡ç·šåŸå§‹è³‡æ–™å¤±æ•—ã€‚")

    # æ–‡æ¹–ç·š
    logger.info("--- å˜—è©¦ç²å–æ–‡æ¹–ç·šè³‡æ–™ ---")
    wenhu_raw_data = metro_soap_api.get_wenhu_car_weight_info()
    if wenhu_raw_data:
        processed_df = process_wenhu_data(wenhu_raw_data)
        existing_df = load_data(WENHU_CONGESTION_FILE)
        combined_df = pd.concat([existing_df, processed_df]).drop_duplicates(subset=['timestamp', 'station_id', 'line_direction_cid'], keep='last').reset_index(drop=True)
        save_data(combined_df, WENHU_CONGESTION_FILE)
        logger.info(f" Â  Â -> âœ… æ–‡æ¹–ç·šè™•ç†å®Œç•¢ã€‚æ–°ç²å– {len(processed_df)} ç­†ï¼Œç›®å‰ç¸½å…± {len(combined_df)} ç­†ã€‚")
    else:
        logger.error(" Â  Â -> âŒ å¾ API ç²å–æ–‡æ¹–ç·šåŸå§‹è³‡æ–™å¤±æ•—ã€‚")

    logger.info("--- [Collector] è³‡æ–™æ”¶é›†å®Œæˆï¼Œç­‰å¾… 5 åˆ†é˜å¾Œä¸‹ä¸€æ¬¡æ›´æ–°... ---")


if __name__ == "__main__":
    while True:
        try:
            collect_and_save_congestion_data()
            time.sleep(5 * 60) # ç­‰å¾… 5 åˆ†é˜
        except KeyboardInterrupt:
            logger.info("--- [Collector] æ”¶åˆ°æ‰‹å‹•ä¸­æ–·æŒ‡ä»¤ï¼Œç¨‹å¼æ­£åœ¨é—œé–‰... ---")
            break
        except Exception as e:
            logger.critical(f"--- âŒ Collector ä¸»è¿´åœˆç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e} ---", exc_info=True)
            logger.info("--- [Collector] å°‡åœ¨ 5 åˆ†é˜å¾Œé‡è©¦... ---\n")
            time.sleep(300)