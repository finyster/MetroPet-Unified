# services/prediction_service.py

import pandas as pd
import xgboost as xgb
import joblib
import os
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, Tuple
import dateparser

# --- è·¯å¾‘è¨­ç½® ---
import sys
SERVICE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SERVICE_DIR)
DATA_DIR = os.path.join(PROJECT_ROOT, 'data')
MODEL_DIR = os.path.join(PROJECT_ROOT, 'model')

if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from services.station_service import StationManager
from services.metro_soap_service import metro_soap_api
import config

# --- é…ç½®æ—¥èªŒ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CongestionPredictor:
    def __init__(self, station_manager_instance: StationManager):
        logger.info("--- [Predictor] æ­£åœ¨åˆå§‹åŒ–äººæµé æ¸¬æœå‹™... ---")
        self.station_manager = station_manager_instance
        self.models: Dict[str, xgb.XGBClassifier] = {}
        self.encoders: Dict[str, any] = {}
        self.scalers: Dict[str, any] = {}
        self.feature_columns: Dict[str, list] = {}
        self.is_ready = self._load_all_models()

        if self.is_ready:
            logger.info("--- âœ… é æ¸¬æœå‹™å·²æˆåŠŸè¼‰å…¥æ¨¡å‹ä¸¦æº–å‚™å°±ç·’ã€‚ ---")
        else:
            logger.error("--- âŒ é æ¸¬æœå‹™åˆå§‹åŒ–å¤±æ•—ï¼Œéƒ¨åˆ†æ¨¡å‹æˆ–æª”æ¡ˆç¼ºå¤±ã€‚ ---")

    def _load_all_models(self) -> bool:
        all_loaded = True
        for line_type in ['high_capacity', 'wenhu']:
            model_path = os.path.join(MODEL_DIR, f'{line_type}_congestion_model.json')
            encoder_path = os.path.join(MODEL_DIR, f'{line_type}_encoder.joblib')
            scaler_path = os.path.join(MODEL_DIR, f'{line_type}_scaler.joblib')
            features_path = os.path.join(MODEL_DIR, f'{line_type}_feature_columns.csv')
            
            if not all(os.path.exists(p) for p in [model_path, encoder_path, scaler_path, features_path]):
                logger.warning(f"--- âš ï¸ åœ¨è·¯å¾‘ '{MODEL_DIR}' ä¸­æ‰¾ä¸åˆ° {line_type} çš„æ¨¡å‹æª”æ¡ˆï¼Œè«‹å…ˆé‹è¡Œ model_trainer.pyã€‚ ---")
                all_loaded = False
                continue
            
            try:
                self.models[line_type] = xgb.XGBClassifier()
                self.models[line_type].load_model(model_path)
                self.encoders[line_type] = joblib.load(encoder_path)
                self.scalers[line_type] = joblib.load(scaler_path)
                self.feature_columns[line_type] = pd.read_csv(features_path)['feature'].tolist()
                logger.info(f"--- âœ… å·²æˆåŠŸå¾ '{MODEL_DIR}' è¼‰å…¥ {line_type} æ¨¡å‹ã€‚ ---")
            except Exception as e:
                logger.error(f"è¼‰å…¥ {line_type} æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
                all_loaded = False
        return all_loaded

    def _get_line_type_and_id(self, station_name: str) -> Optional[Tuple[str, str]]:
        station_ids = self.station_manager.get_station_ids(station_name)
        if not station_ids:
            logger.warning(f"ç„¡æ³•åœ¨ StationManager ä¸­æ‰¾åˆ°ç«™å '{station_name}' çš„ä»»ä½• IDã€‚")
            return None, None
        station_id = station_ids[0]
        if station_id.startswith('BR'):
            return 'wenhu', station_id
        return 'high_capacity', station_id

    def _create_prediction_features(self, station_id: str, line_direction_cid: int, line_type: str, target_datetime: datetime) -> pd.DataFrame:
        """
        æ ¹æ“šæŒ‡å®šçš„æ—¥æœŸæ™‚é–“ï¼Œå‰µå»ºæ¨¡å‹æ‰€éœ€çš„ç‰¹å¾µã€‚
        """
        with open(os.path.join(DATA_DIR, 'mrt_station_info.json'), 'r', encoding='utf-8') as f:
            station_info = json.load(f)
        transfer_stations = {sid for info in station_info.values() if isinstance(info, dict) for sid in info.get('station_ids', []) if info.get('is_transfer')}
        
        # --- ã€é—œéµä¿®æ­£ã€‘æ ¹æ“šæ™‚é–“æ®µæ¨¡æ“¬æ›´åˆç†çš„æ»¯å¾Œæ“æ“ åº¦å€¼ ---
        # é€™è£¡æ ¹æ“šæ™‚é–“æ®µå’Œæ˜¯å¦ç‚ºå°–å³°æ™‚æ®µï¼Œçµ¦å‡ºä¸€å€‹æ›´åˆç†çš„é è¨­å€¼
        lag_5min_congestion = 0.0
        lag_1hr_congestion = 0.0
        
        # æ¨¡æ“¬å°–å³°æ™‚æ®µçš„æ“æ“ åº¦
        if target_datetime.weekday() < 5 and target_datetime.hour in [7, 8, 17, 18]:
            lag_5min_congestion = 1.5 
            lag_1hr_congestion = 2.0  
        # æ¨¡æ“¬é€±æœ«çš„æ“æ“ åº¦
        elif target_datetime.weekday() >= 5:
            lag_5min_congestion = 1.0 
            lag_1hr_congestion = 1.0
        
        # æ¨¡æ“¬å¤œé–“æˆ–é›¢å³°æ™‚æ®µçš„æ“æ“ åº¦
        if target_datetime.hour in [21, 22, 23, 0, 1, 2, 3, 4, 5]:
             lag_5min_congestion = 0.5
             lag_1hr_congestion = 0.5
        
        num_cars = 4 if line_type == 'wenhu' else 6
        records = []
        for car_num in range(1, num_cars + 1):
            records.append({
                'station_id': station_id,
                'line_direction_cid': str(line_direction_cid),
                'hour': target_datetime.hour,
                'minute': target_datetime.minute,
                'day_of_week': target_datetime.weekday(),
                'is_weekend': int(target_datetime.weekday() >= 5),
                'is_peak_hour': int(target_datetime.hour in [7, 8, 17, 18, 19]),
                'is_transfer_station': int(station_id in transfer_stations),
                'car_number': car_num,
                'lag_5min_congestion': lag_5min_congestion,
                'lag_1hr_congestion': lag_1hr_congestion
            })
        
        df_raw = pd.DataFrame(records)
        
        encoder = self.encoders[line_type]
        categorical_features = ['station_id', 'line_direction_cid']
        encoded_data = encoder.transform(df_raw[categorical_features])
        encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_features))
        
        numeric_features = [
            'hour', 'minute', 'day_of_week', 'is_weekend', 'is_peak_hour', 'is_transfer_station',
            'car_number', 'lag_5min_congestion', 'lag_1hr_congestion'
        ]
        
        final_df = pd.concat([df_raw[numeric_features].reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)
        
        scaler = self.scalers[line_type]
        final_df[numeric_features] = scaler.transform(final_df[numeric_features])
        
        final_df = final_df.reindex(columns=self.feature_columns[line_type], fill_value=0)
        
        return final_df

    def predict_for_station(self, station_name: str, direction: str, target_datetime: datetime) -> Dict[str, Any]:
        """
        ç‚ºæŒ‡å®šè»Šç«™å’Œæ–¹å‘æä¾›é€šç”¨çš„è»Šå»‚æ“æ“ åº¦é æ¸¬ã€‚
        ç¾åœ¨å¯ä»¥æ ¹æ“šæŒ‡å®šçš„ `target_datetime` é€²è¡Œé æ¸¬ã€‚
        """
        if not self.is_ready:
            return {"error": "é æ¸¬æœå‹™å°šæœªæº–å‚™å°±ç·’ï¼Œè«‹æª¢æŸ¥æ¨¡å‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨ã€‚"}

        line_type, station_id = self._get_line_type_and_id(station_name)
        if not line_type:
            return {"error": f"ç„¡æ³•è­˜åˆ¥è»Šç«™ '{station_name}'ï¼Œè«‹ç¢ºèªç«™åæ˜¯å¦æ­£ç¢ºã€‚"}
            
        direction_map = {"ä¸Šè¡Œ": 1, "å¾€å—æ¸¯å±•è¦½é¤¨": 1, "å¾€å‹•ç‰©åœ’": 1, "å¾€è¿´é¾": 1, "å¾€è˜†æ´²": 1, "å¾€æ·¡æ°´":1, "å¾€åŒ—æŠ•":1, "ä¸‹è¡Œ": 2, "å¾€é ‚åŸ”": 2, "å¾€è±¡å±±": 2, "å¾€å¤§å®‰":2, "å¾€å—å‹¢è§’":2, "å¾€æ–°åº—": 2, "å¾€å°é›»å¤§æ¨“":2, "å¾€æ¿æ©‹":2}
        line_direction_cid = direction_map.get(direction, 1)

        logger.info(f"é–‹å§‹ç‚ºè»Šç«™ '{station_name}' (ID: {station_id}, æ–¹å‘: {line_direction_cid}) æ–¼ {target_datetime.strftime('%Y-%m-%d %H:%M')} é€²è¡Œé æ¸¬...")
        
        try:
            X_pred = self._create_prediction_features(station_id, line_direction_cid, line_type, target_datetime)
            model = self.models[line_type]
            predictions = model.predict(X_pred)
            
            # ã€é—œéµä¿®æ­£ã€‘é€™è£¡å°é æ¸¬çµæœé€²è¡Œèª¿æ•´ï¼Œè®“å®ƒæ›´è²¼è¿‘ç¾å¯¦ï¼Œä¸åªæ˜¯èˆ’é©
            # é€™æ˜¯ç‚ºäº†æ‡‰å°æ¨¡å‹åœ¨ç°¡å–®ç‰¹å¾µä¸‹å¯èƒ½ç¼ºä¹è®ŠåŒ–çš„å•é¡Œ
            congestion_map = {0: "èˆ’é©", 1: "æ­£å¸¸", 2: "ç•¥å¤š", 3: "æ“æ“ "} # XGBoost é¡åˆ¥å¾ 0 é–‹å§‹
            results = []
            for i, pred_class in enumerate(predictions):
                # é€™è£¡å¯ä»¥æ ¹æ“šé æ¸¬æ™‚é–“åšä¸€äº›ç°¡å–®çš„å¾Œè™•ç†ï¼Œå¢åŠ è®Šå‹•æ€§
                # ä¾‹å¦‚ï¼Œå¦‚æœé æ¸¬æ™‚é–“åœ¨å°–å³°æ™‚æ®µï¼Œå³ä½¿æ¨¡å‹é æ¸¬èˆ’é©ï¼Œä¹Ÿå°‡å…¶èª¿æ•´ç‚ºæ­£å¸¸
                level = int(pred_class)
                
                if target_datetime.weekday() < 5 and target_datetime.hour in [7, 8, 17, 18]:
                     if level == 0: level = 1 # å°–å³°æ™‚æ®µè‡³å°‘æ˜¯æ­£å¸¸
                
                results.append({
                    "car_number": i + 1,
                    "congestion_level": level + 1, # è½‰æ›å› 1,2,3,4 çš„ç­‰ç´š
                    "congestion_text": congestion_map.get(level, "æœªçŸ¥")
                })

            return {
                "station_name": station_name,
                "direction": direction,
                "prediction_time": target_datetime.isoformat(),
                "congestion_by_car": results
            }

        except Exception as e:
            logger.error(f"ç‚º '{station_name}' é€²è¡Œé æ¸¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return {"error": f"é æ¸¬æ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥æ—¥èªŒã€‚"}

    def predict_next_train_congestion(self, station_name: str, direction: str) -> Dict[str, Any]:
        """
        çµåˆå³æ™‚åˆ—è»Šè³‡è¨Šèˆ‡æ“æ“ åº¦é æ¸¬æ¨¡å‹ï¼Œç‚ºä½¿ç”¨è€…æä¾›å³å°‡åˆ°ç«™åˆ—è»Šçš„é æ¸¬çµæœã€‚
        """
        if not self.is_ready:
            return {"error": "é æ¸¬æœå‹™å°šæœªæº–å‚™å°±ç·’ï¼Œè«‹æª¢æŸ¥æ¨¡å‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨ã€‚"}

        logger.info(f"--- ğŸš€ æ­£åœ¨å¾ Metro API ç²å–å³æ™‚åˆ—è»Šè³‡è¨Šä»¥æŸ¥æ‰¾è»Šç«™ '{station_name}' å¾€ '{direction}' æ–¹å‘ ---")
        try:
            all_train_info = metro_soap_api.get_realtime_track_info()
        except Exception as e:
            logger.error(f"ç²å–å³æ™‚åˆ—è»Šè³‡è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return {"error": "ç„¡æ³•å¾ Metro API ç²å–å³æ™‚åˆ—è»Šè³‡è¨Šï¼Œè«‹æª¢æŸ¥æœå‹™é€£ç·šã€‚"}

        congestion_prediction_for_station = self.predict_for_station(station_name, direction, target_datetime=datetime.now())

        if "error" in congestion_prediction_for_station:
            return {"error": congestion_prediction_for_station["error"]}

        relevant_trains = []
        if all_train_info:
            for train in all_train_info:
                if direction in train.get('DestinationName', ''):
                    relevant_trains.append(train)

            def parse_countdown_to_seconds(countdown_str):
                if countdown_str == 'åˆ—è»Šé€²ç«™':
                    return 0
                if 'åˆ†' in countdown_str and 'ç§’' in countdown_str:
                    parts = countdown_str.replace(' åˆ†é˜ ', ' ').replace(' ç§’', '').split(' ')
                    if len(parts) == 2:
                        try:
                            minutes = int(parts[0])
                            seconds = int(parts[1])
                            return minutes * 60 + seconds
                        except ValueError:
                            return float('inf')
                return float('inf')

            relevant_trains.sort(key=lambda x: parse_countdown_to_seconds(x.get('CountDown', 'æœªçŸ¥')))

        return {
            "station_name": station_name,
            "direction": direction,
            "prediction_time": datetime.now().isoformat(),
            "relevant_trains_info": relevant_trains,
            "congestion_prediction_for_station": congestion_prediction_for_station
        }